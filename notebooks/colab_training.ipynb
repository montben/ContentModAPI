{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/montben/ContentModAPI/blob/main/notebooks/colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Muzzle Content Moderation - Model Training\n",
        "\n",
        "This notebook trains a BERT model for multi-label content moderation using your collected datasets.\n",
        "\n",
        "## Training Pipeline:\n",
        "1. 📊 Merge and prepare datasets\n",
        "2. 🤖 Fine-tune BERT model \n",
        "3. 📈 Evaluate performance\n",
        "4. 💾 Save trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prerequisites: Run colab_setup.ipynb first!\n",
        "# This assumes your project is already loaded and dependencies installed\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Make sure we're in the right directory\n",
        "if not os.path.exists('scripts'):\n",
        "    print(\"📁 Changing to project directory...\")\n",
        "    os.chdir('/content/ContentModAPI')\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = os.getcwd()\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"📁 Working directory: {os.getcwd()}\")\n",
        "print(f\"📂 Contents: {os.listdir('.')}\")\n",
        "\n",
        "# Import your project modules\n",
        "from scripts.preprocessing.label_schema import (\n",
        "    LABEL_SCHEMA, LABEL_NAMES, LABEL_DESCRIPTIONS,\n",
        "    create_label_vector, vector_to_labels\n",
        ")\n",
        "from scripts.training.train_bert import MultiLabelTrainer\n",
        "\n",
        "print(\"✅ Imports successful!\")\n",
        "print(f\"🚀 GPU available: {torch.cuda.is_available()}\")\n",
        "print(f\"🎯 Ready for training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Preparation\n",
        "\n",
        "First, let's merge all our collected datasets into a unified training format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the merge_datasets.py functionality inline for Colab\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "def merge_datasets():\n",
        "    \"\"\"Merge all collected datasets into unified training format.\"\"\"\n",
        "\n",
        "    data_dir = Path(\"data/datasets\")\n",
        "    all_data = []\n",
        "\n",
        "    # Dataset directories to process\n",
        "    dataset_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
        "    print(f\"Found datasets: {[d.name for d in dataset_dirs]}\")\n",
        "\n",
        "    for dataset_dir in dataset_dirs:\n",
        "        processed_file = dataset_dir / \"processed_data.csv\"\n",
        "        if processed_file.exists():\n",
        "            print(f\"Loading {dataset_dir.name}...\")\n",
        "            df = pd.read_csv(processed_file)\n",
        "            df['dataset_source'] = dataset_dir.name\n",
        "            all_data.append(df)\n",
        "            print(f\"  ✅ {len(df):,} samples from {dataset_dir.name}\")\n",
        "        else:\n",
        "            print(f\"  ❌ No processed_data.csv in {dataset_dir.name}\")\n",
        "\n",
        "    if not all_data:\n",
        "        raise Exception(\"No datasets found! Run data collection first.\")\n",
        "\n",
        "    # Combine all datasets\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"\\n📊 Combined dataset: {len(combined_df):,} total samples\")\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    label_columns = list(LABEL_SCHEMA.keys())\n",
        "    missing_cols = [col for col in label_columns if col not in combined_df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"⚠️  Adding missing label columns: {missing_cols}\")\n",
        "        for col in missing_cols:\n",
        "            combined_df[col] = False\n",
        "\n",
        "    # Create train/val/test splits\n",
        "    print(\"Creating train/val/test splits...\")\n",
        "\n",
        "    # First split: 80% train, 20% temp\n",
        "    train_df, temp_df = train_test_split(\n",
        "        combined_df, test_size=0.2, random_state=42,\n",
        "        stratify=combined_df['dataset_source']\n",
        "    )\n",
        "\n",
        "    # Second split: 10% val, 10% test from the temp 20%\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df, test_size=0.5, random_state=42,\n",
        "        stratify=temp_df['dataset_source']\n",
        "    )\n",
        "\n",
        "    # Add split column\n",
        "    train_df = train_df.copy()\n",
        "    val_df = val_df.copy()\n",
        "    test_df = test_df.copy()\n",
        "\n",
        "    train_df['split'] = 'train'\n",
        "    val_df['split'] = 'val'\n",
        "    test_df['split'] = 'test'\n",
        "\n",
        "    # Combine back\n",
        "    final_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "\n",
        "    print(f\"📈 Splits created:\")\n",
        "    print(f\"  Train: {len(train_df):,} ({len(train_df)/len(final_df)*100:.1f}%)\")\n",
        "    print(f\"  Val:   {len(val_df):,} ({len(val_df)/len(final_df)*100:.1f}%)\")\n",
        "    print(f\"  Test:  {len(test_df):,} ({len(test_df)/len(final_df)*100:.1f}%)\")\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# Execute the merge\n",
        "merged_data = merge_datasets()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
