{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/montben/ContentModAPI/blob/main/notebooks/simplified_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Simplified Content Moderation Training\n",
        "\n",
        "This notebook trains a BERT model using a **single comprehensive dataset** (Civil Comments).\n",
        "\n",
        "**üöÄ Best run on Google Colab for free GPU!**\n",
        "\n",
        "## What Changed?\n",
        "- ‚úÖ **ONE dataset** instead of 5+ mixed datasets\n",
        "- ‚úÖ **6 labels** instead of 8 (removed self_harm and spam)\n",
        "- ‚úÖ **Consistent labeling** from a single source\n",
        "- ‚úÖ **Better training** with cleaner data\n",
        "\n",
        "## Labels:\n",
        "1. Toxicity - General toxic language\n",
        "2. Hate Speech - Identity-based attacks\n",
        "3. Harassment - Personal attacks\n",
        "4. Violence - Threats and violent content\n",
        "5. Sexual - Sexual content\n",
        "6. Profanity - Explicit language\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Colab Setup Instructions\n",
        "\n",
        "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
        "2. **Run all cells** in order\n",
        "3. Training will take ~30-45 minutes with GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# STEP 0: Clone Repository (Colab Only)\n",
        "# ====================================\n",
        "# Run this cell if you're on Colab\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if we're in Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"üîµ Running on Google Colab\")\n",
        "\n",
        "    # Clone the repository if not already cloned\n",
        "    if not os.path.exists('/content/ContentModAPI'):\n",
        "        print(\"üì• Cloning repository...\")\n",
        "        !git clone https://github.com/YOUR_GITHUB_USERNAME/ContentModAPI.git /content/ContentModAPI\n",
        "        print(\"‚úÖ Repository cloned!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Repository already cloned!\")\n",
        "\n",
        "    # Change to project directory\n",
        "    os.chdir('/content/ContentModAPI')\n",
        "    print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "\n",
        "    # Install dependencies\n",
        "    print(\"\\nüì¶ Installing dependencies...\")\n",
        "    !pip install -q transformers datasets torch scikit-learn pandas numpy matplotlib seaborn\n",
        "    print(\"‚úÖ Dependencies installed!\")\n",
        "else:\n",
        "    print(\"üü¢ Running locally\")\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"\\nüöÄ GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  No GPU detected. Training will be slow.\")\n",
        "    print(\"   üí° In Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# STEP 1: Import Libraries\n",
        "# ====================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add project to path\n",
        "project_root = os.getcwd()\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(\"‚úÖ All imports loaded!\")\n",
        "print(f\"üìÅ Working from: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Download Dataset\n",
        "\n",
        "Download Civil Comments dataset from HuggingFace (~5-10 minutes for full dataset).\n",
        "\n",
        "**Options:**\n",
        "- `sample_size=10000` - Quick test (recommended for first run)\n",
        "- `sample_size=None` - Full dataset (~300k samples, best results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# STEP 2: Download Civil Comments Dataset\n",
        "# ====================================\n",
        "\n",
        "from scripts.data_collection.download_civil_comments import download_civil_comments\n",
        "\n",
        "# Check if already downloaded\n",
        "dataset_path = Path(\"data/datasets/civil_comments/processed_data.csv\")\n",
        "\n",
        "if not dataset_path.exists():\n",
        "    print(\"üì• Downloading Civil Comments dataset from HuggingFace...\")\n",
        "    print(\"This will take ~5-10 minutes depending on sample size...\")\n",
        "    print()\n",
        "\n",
        "    # Download dataset with BALANCED sampling\n",
        "    # This gives you 60% toxic, 40% safe = much more efficient training!\n",
        "    download_civil_comments(\n",
        "        output_dir=\"data/datasets\",\n",
        "        sample_size=15000,  # With balanced=True, 15k is plenty!\n",
        "        balanced=True  # ‚≠ê This is the magic - balances toxic vs safe samples\n",
        "    )\n",
        "    print(\"\\n‚úÖ Download complete!\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset already downloaded!\")\n",
        "\n",
        "# Show what we have\n",
        "if dataset_path.exists():\n",
        "    df_check = pd.read_csv(dataset_path)\n",
        "    print(f\"\\nüìä Dataset ready: {len(df_check):,} samples\")\n",
        "\n",
        "    # Show toxic vs safe distribution\n",
        "    label_cols = ['toxicity', 'hate_speech', 'harassment', 'violence', 'sexual', 'profanity']\n",
        "    has_any_label = df_check[label_cols].any(axis=1)\n",
        "    print(f\"   Toxic samples: {has_any_label.sum():,} ({has_any_label.sum()/len(df_check)*100:.1f}%)\")\n",
        "    print(f\"   Safe samples: {(~has_any_label).sum():,} ({(~has_any_label).sum()/len(df_check)*100:.1f}%)\")\n",
        "    del df_check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load and Explore Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"data/datasets/civil_comments/processed_data.csv\")\n",
        "\n",
        "print(f\"üìä Dataset size: {len(df):,} samples\")\n",
        "print(f\"\\nüìã Columns: {list(df.columns)}\")\n",
        "print(f\"\\nüîç First few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label distribution\n",
        "from scripts.preprocessing.label_schema import LABEL_SCHEMA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_columns = list(LABEL_SCHEMA.keys())\n",
        "label_counts = df[label_columns].sum()\n",
        "\n",
        "print(\"üìä Label Distribution:\")\n",
        "for label, count in label_counts.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"  {label}: {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Label Distribution - Civil Comments Dataset')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Model\n",
        "\n",
        "Use the training script to train BERT on this single dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Train/Val/Test Splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# STEP 4: Create Data Splits\n",
        "# ====================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create splits: 80% train, 10% val, 10% test\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Add split column\n",
        "train_df = train_df.copy()\n",
        "val_df = val_df.copy()\n",
        "test_df = test_df.copy()\n",
        "\n",
        "train_df['split'] = 'train'\n",
        "val_df['split'] = 'val'\n",
        "test_df['split'] = 'test'\n",
        "\n",
        "# Combine back\n",
        "final_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "\n",
        "print(f\"üìà Data Split:\")\n",
        "print(f\"  Train: {len(train_df):6,} samples ({len(train_df)/len(final_df)*100:.1f}%)\")\n",
        "print(f\"  Val:   {len(val_df):6,} samples ({len(val_df)/len(final_df)*100:.1f}%)\")\n",
        "print(f\"  Test:  {len(test_df):6,} samples ({len(test_df)/len(final_df)*100:.1f}%)\")\n",
        "\n",
        "# Save to disk\n",
        "output_file = \"data/datasets/civil_comments_with_splits.csv\"\n",
        "final_df.to_csv(output_file, index=False)\n",
        "print(f\"\\nüíæ Saved split dataset to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train BERT Model\n",
        "\n",
        "Now we'll train the BERT model on our dataset. This will take ~30-45 minutes with GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# STEP 5: Train BERT Model\n",
        "# ====================================\n",
        "\n",
        "from scripts.training.train_bert import MultiLabelTrainer\n",
        "\n",
        "# Initialize trainer\n",
        "print(\"ü§ñ Initializing BERT model...\")\n",
        "trainer = MultiLabelTrainer(\n",
        "    model_name=\"bert-base-uncased\",\n",
        "    num_labels=6  # Our 6-label schema\n",
        ")\n",
        "\n",
        "# Load model\n",
        "trainer.load_model()\n",
        "print(\"‚úÖ Model loaded!\")\n",
        "print(f\"   Model: {trainer.model_name}\")\n",
        "print(f\"   Labels: {trainer.num_labels}\")\n",
        "print(f\"   Parameters: {sum(p.numel() for p in trainer.model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset for training\n",
        "print(\"üìä Preparing dataset...\")\n",
        "dataset = trainer.prepare_dataset(output_file)\n",
        "print(\"‚úÖ Dataset prepared!\")\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "for split, data in dataset.items():\n",
        "    print(f\"  {split:12} {len(data):6,} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "config = {\n",
        "    'epochs': 2,  # Start with 2 epochs for testing\n",
        "    'batch_size': 16,  # Reduce to 8 if you run out of GPU memory\n",
        "    'eval_batch_size': 32,\n",
        "    'learning_rate': 2e-5,\n",
        "    'warmup_steps': 500,\n",
        "    'weight_decay': 0.01,\n",
        "    'use_wandb': False  # Set to True if you want experiment tracking\n",
        "}\n",
        "\n",
        "output_dir = \"artifacts/models/bert-multilabel\"\n",
        "\n",
        "print(\"üèãÔ∏è Starting training...\")\n",
        "print(f\"Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"\\nOutput directory: {output_dir}\")\n",
        "print(\"\\n‚è∞ This will take ~30-45 minutes with GPU...\")\n",
        "print(\"‚òï Grab some coffee!\\n\")\n",
        "\n",
        "# Train the model\n",
        "trained_model = trainer.train(dataset, output_dir, config)\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluate Model\n",
        "\n",
        "Let's see how well our model performed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# STEP 6: Evaluate Model\n",
        "# ====================================\n",
        "\n",
        "print(\"üìä Evaluating on test set...\")\n",
        "test_results = trained_model.evaluate(dataset['test'])\n",
        "\n",
        "print(\"\\nüìà Test Results:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show F1 scores\n",
        "f1_metrics = {k: v for k, v in test_results.items() if 'f1' in k}\n",
        "for metric, value in sorted(f1_metrics.items()):\n",
        "    metric_name = metric.replace('eval_', '')\n",
        "    stars = \"‚≠ê\" * int(value * 5)  # Visual rating\n",
        "    print(f\"{metric_name:20} {value:.4f} {stars}\")\n",
        "\n",
        "# Save results\n",
        "import json\n",
        "results_path = Path(output_dir) / \"test_results.json\"\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(test_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Results saved to: {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test the Model\n",
        "\n",
        "Try the model on some example texts!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================\n",
        "# STEP 7: Test the Model\n",
        "# ====================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load trained model\n",
        "print(\"üì¶ Loading trained model...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model.eval()\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(f\"‚úÖ Model loaded on: {device}\")\n",
        "\n",
        "def predict(text):\n",
        "    \"\"\"Predict labels for a given text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.sigmoid(outputs.logits)\n",
        "\n",
        "    # Get predictions\n",
        "    results = {}\n",
        "    for i, label in enumerate(LABEL_SCHEMA.keys()):\n",
        "        prob = predictions[0][i].item()\n",
        "        results[label] = {\n",
        "            'probability': prob,\n",
        "            'flagged': prob > 0.5\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"I love this community, everyone is so helpful and kind!\",\n",
        "    \"You're a complete idiot and should just shut up\",\n",
        "    \"I disagree with your opinion but respect your viewpoint\",\n",
        "    \"This is absolute garbage and you're an incompetent moron\",\n",
        "]\n",
        "\n",
        "print(\"\\nüß™ Testing model on example texts:\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    print(f\"\\nüìù Example {i}:\")\n",
        "    print(f\"   Text: \\\"{text}\\\"\")\n",
        "    results = predict(text)\n",
        "\n",
        "    flagged_labels = [label for label, info in results.items() if info['flagged']]\n",
        "\n",
        "    if flagged_labels:\n",
        "        print(f\"   ‚ö†Ô∏è  FLAGGED: {', '.join(flagged_labels)}\")\n",
        "        for label in flagged_labels:\n",
        "            print(f\"      - {label}: {results[label]['probability']:.3f}\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ SAFE (no flags)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For full training, use the command line:\n",
        "# python scripts/training/train_bert.py --data-path data/datasets/civil_comments/processed_data.csv\n",
        "\n",
        "print(\"‚úÖ Training simplified!\")\n",
        "print(\"\\nTo train the model, run:\")\n",
        "print(\"  python scripts/training/train_bert.py \\\\\")\n",
        "print(\"    --data-path data/datasets/civil_comments/processed_data.csv \\\\\")\n",
        "print(\"    --epochs 3 \\\\\")\n",
        "print(\"    --batch-size 16\")\n",
        "print(\"\\nThis will train a BERT model on the Civil Comments dataset.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
